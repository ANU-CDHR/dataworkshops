{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Getting data via Twitter APIs\n",
    "======\n",
    "**Get twitter data and save in files.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the following examples, we showcase how to use both Twitter Search API and Twitter Streaming API to get data. Twitter Search API returns data already exists, while Twitter streaming API returns data from tweets as they happens in real time. \"With a specific keyword, you can typically only poll the last 5,000 tweets per keyword. You are further limited by the number of requests you can make in a certain time period. The Twitter request limits have changed over the years but are currently limited to 180 requests in a 15 minute period.\" Read more at post: https://brightplanet.com/2013/06/25/twitter-firehose-vs-twitter-api-whats-the-difference-and-why-should-you-care/ .**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import tweepy and other required python libraries for getting data from Twitter.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The following Python modules need to be installed if they are not in the environment (Run all to test if any module is missing) in order to run this notebook (run without ! under command line or with ! in the notebook, for SWAN, see https://support.aarnet.edu.au/hc/en-us/articles/360000668076-How-do-I-add-code-libraries-to-my-Notebook-):_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out to run install, and install other modules with !pip if report missing, may need to shutdown/restart Kernel session in SWAN\n",
    "#!pip install tweepy\n",
    "#!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tweepy and other required python libraries\n",
    "import tweepy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Twitter developer key and secret from https://developer.twitter.com and enter the information below. See also https://www.slickremix.com/docs/how-to-get-api-keys-and-tokens-for-twitter/.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set twitter API information\n",
    "consumer_key = '#######'\n",
    "consumer_secret = '#######'\n",
    "access_token = '#######'\n",
    "access_token_secret = '#######'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Tweepy API object from key and secret.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Tweepy API object\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create DataFrame to save twitter data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrame from twitter data\n",
    "df = pd.DataFrame(columns = ['text', 'user.name', 'user.statuses_count','user.screen_name',\n",
    "                             'user.followers_count', 'user.location', 'user.verified','user.profile_image_url_https',\n",
    "                             'favorite_count', 'retweet_count', 'created_at','hashtags','quoted_hashtags','user_lat','user_lng'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare library and test to get gps information (latitude, longitude) from twitter data location.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare library to get gps information (latitude, longitude) from twitter data location \n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"test\",timeout=10)\n",
    "\n",
    "#loc = geolocator.geocode(\"New York, NY\")\n",
    "#loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.1 Use Twitter Search API to get data\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set maximum number of tweets to get. Create the steam function to get twitter data and save in CSV file.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example to export as single csv file for small dataset.**\n",
    "Read more at: http://docs.tweepy.org/en/latest/api.html#API.search Note: count – The number of tweets to return per page, currently up to a maximum of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# search function to get twitter data and save in csv, data-searchterms, file_name-csv file name to save, tweet_number-max number of tweets to get\n",
    "def search_twitter(data, file_name, tweet_number=200):\n",
    "    i = 0\n",
    "    for tweet in tweepy.Cursor(api.search, q=data, count=100, lang='en').items():\n",
    "        hashtags=[]\n",
    "        quoted_hashtags=[]\n",
    "        #check if hashtags exist\n",
    "        if tweet.entities.get(\"hashtags\")!=[]:\n",
    "            hashtags = '['+','.join(pd.json_normalize(tweet.entities.get(\"hashtags\"))['text'].tolist())+']'\n",
    "        #check if it is quoted tweet and if hashtags exist in the quoted tweet\n",
    "        if hasattr(tweet, 'quoted_status') and tweet.quoted_status.entities.get(\"hashtags\")!=[]:\n",
    "            quoted_hashtags = '['+','.join(pd.json_normalize(tweet.quoted_status.entities.get(\"hashtags\"))['text'].tolist())+']'\n",
    "        df.loc[i, 'text'] = tweet.text\n",
    "        df.loc[i, 'user.name'] = tweet.user.name\n",
    "        df.loc[i, 'user.statuses_count'] = tweet.user.statuses_count\n",
    "        df.loc[i, 'user.screen_name'] = tweet.user.screen_name\n",
    "        df.loc[i, 'user.followers_count'] = tweet.user.followers_count\n",
    "        df.loc[i, 'user.location'] = tweet.user.location\n",
    "        df.loc[i, 'user.verified'] = tweet.user.verified\n",
    "        df.loc[i, 'user.profile_image_url_https'] = tweet.user.profile_image_url_https\n",
    "        df.loc[i, 'favorite_count'] = tweet.favorite_count\n",
    "        df.loc[i, 'retweet_count'] = tweet.retweet_count\n",
    "        df.loc[i, 'created_at'] = tweet.created_at\n",
    "        df.loc[i, 'hashtags']=hashtags\n",
    "        df.loc[i, 'quoted_hashtags']=quoted_hashtags\n",
    "        # process user location, check if user location exists first and then get lat/lng if it does\n",
    "        if tweet.user.location is not None:\n",
    "            coord = geolocator.geocode(tweet.user.location)\n",
    "            if coord is not None:\n",
    "                df.loc[i, 'user_lat'] = coord.latitude\n",
    "                df.loc[i, 'user_lng'] = coord.longitude\n",
    "        # save data frame to csv with the file_name\n",
    "        df.to_csv('{}.csv'.format(file_name))\n",
    "        i+=1\n",
    "        # break the search if i reaches tweet_number - max number of tweets to get, break if it has, keep searching if it hasn't\n",
    "        if i == tweet_number:\n",
    "            break\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set search terms for tweets and CSV file name to save the returned tweets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum tweets to get\n",
    "#tweet_number=200\n",
    "# set twitter query for getting data, file_name to save data\n",
    "search_twitter(data = ['digitalhumanities or #digitalhumanities'], file_name = 'dh_tweets')\n",
    "# try different search terms and tweet number\n",
    "#stream(data = ['covid or #covid'], file_name = 'cv_tweets', tweet_number=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check and inspect the DataFrame.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>user.name</th>\n",
       "      <th>user.statuses_count</th>\n",
       "      <th>user.screen_name</th>\n",
       "      <th>user.followers_count</th>\n",
       "      <th>user.location</th>\n",
       "      <th>user.verified</th>\n",
       "      <th>user.profile_image_url_https</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>created_at</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>quoted_hashtags</th>\n",
       "      <th>user_lat</th>\n",
       "      <th>user_lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @_epidat: (2/3) 38k inscriptions from ~220 ...</td>\n",
       "      <td>Bot do Laboratório de Humanidades Digitais da ...</td>\n",
       "      <td>1219</td>\n",
       "      <td>BotLabhd</td>\n",
       "      <td>59</td>\n",
       "      <td>LABHDUFBA</td>\n",
       "      <td>False</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/129869437...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-31 22:56:37</td>\n",
       "      <td>[Jewish,cemeteries,epigraphic]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(2/3) 38k inscriptions from ~220 #Jewish #ceme...</td>\n",
       "      <td>epidat</td>\n",
       "      <td>3248</td>\n",
       "      <td>_epidat</td>\n",
       "      <td>332</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/916020636...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-31 22:56:31</td>\n",
       "      <td>[Jewish,cemeteries,epigraphic]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @helenkbones: If you've heard me going on a...</td>\n",
       "      <td>Bot do Laboratório de Humanidades Digitais da ...</td>\n",
       "      <td>1219</td>\n",
       "      <td>BotLabhd</td>\n",
       "      <td>59</td>\n",
       "      <td>LABHDUFBA</td>\n",
       "      <td>False</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/129869437...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2021-01-31 22:18:02</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @juliannenyhan: I'm very excited for @ianmi...</td>\n",
       "      <td>Andryah Ayotte</td>\n",
       "      <td>10</td>\n",
       "      <td>AndryahAyotte</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/135493179...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2021-01-29 05:48:34</td>\n",
       "      <td>[digitalhumanities]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @juliannenyhan: I'm very excited for @ianmi...</td>\n",
       "      <td>eResearch UCL</td>\n",
       "      <td>12028</td>\n",
       "      <td>eResearch_UCL</td>\n",
       "      <td>2992</td>\n",
       "      <td>London, England</td>\n",
       "      <td>False</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/731053704...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2021-01-27 17:59:33</td>\n",
       "      <td>[digitalhumanities]</td>\n",
       "      <td>[]</td>\n",
       "      <td>51.5073</td>\n",
       "      <td>-0.127647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  RT @_epidat: (2/3) 38k inscriptions from ~220 ...   \n",
       "1  (2/3) 38k inscriptions from ~220 #Jewish #ceme...   \n",
       "2  RT @helenkbones: If you've heard me going on a...   \n",
       "3  RT @juliannenyhan: I'm very excited for @ianmi...   \n",
       "4  RT @juliannenyhan: I'm very excited for @ianmi...   \n",
       "\n",
       "                                           user.name user.statuses_count  \\\n",
       "0  Bot do Laboratório de Humanidades Digitais da ...                1219   \n",
       "1                                             epidat                3248   \n",
       "2  Bot do Laboratório de Humanidades Digitais da ...                1219   \n",
       "3                                     Andryah Ayotte                  10   \n",
       "4                                      eResearch UCL               12028   \n",
       "\n",
       "  user.screen_name user.followers_count    user.location user.verified  \\\n",
       "0         BotLabhd                   59        LABHDUFBA         False   \n",
       "1          _epidat                  332                          False   \n",
       "2         BotLabhd                   59        LABHDUFBA         False   \n",
       "3    AndryahAyotte                    0                          False   \n",
       "4    eResearch_UCL                 2992  London, England         False   \n",
       "\n",
       "                        user.profile_image_url_https favorite_count  \\\n",
       "0  https://pbs.twimg.com/profile_images/129869437...              0   \n",
       "1  https://pbs.twimg.com/profile_images/916020636...              1   \n",
       "2  https://pbs.twimg.com/profile_images/129869437...              0   \n",
       "3  https://pbs.twimg.com/profile_images/135493179...              0   \n",
       "4  https://pbs.twimg.com/profile_images/731053704...              0   \n",
       "\n",
       "  retweet_count           created_at                        hashtags  \\\n",
       "0             1  2021-01-31 22:56:37  [Jewish,cemeteries,epigraphic]   \n",
       "1             1  2021-01-31 22:56:31  [Jewish,cemeteries,epigraphic]   \n",
       "2            11  2021-01-31 22:18:02                              []   \n",
       "3             7  2021-01-29 05:48:34             [digitalhumanities]   \n",
       "4             7  2021-01-27 17:59:33             [digitalhumanities]   \n",
       "\n",
       "  quoted_hashtags user_lat  user_lng  \n",
       "0              []      NaN       NaN  \n",
       "1              []      NaN       NaN  \n",
       "2              []      NaN       NaN  \n",
       "3              []      NaN       NaN  \n",
       "4              []  51.5073 -0.127647  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example to export as multiple csv files for large dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set maximum tweets to get\n",
    "tweet_number=5000\n",
    "# search function to get twitter data and save in csv, tweet_number-maximum tweets to get, tweetsperfile-number of tweets per file\n",
    "def search_twitter_split(data, file_name, tweet_number=5000, tweetsperfile=1000):\n",
    "    tweet_count = 0 # total tweets count\n",
    "    filetweet_count = 0 # tweets in file count\n",
    "    fileno = 0 # file number count\n",
    "    # create DataFrame from twitter data\n",
    "    df = pd.DataFrame(columns = ['text', 'user.name', 'user.statuses_count','user.screen_name',\n",
    "                             'user.followers_count', 'user.location', 'user.verified','user.profile_image_url_https',\n",
    "                             'favorite_count', 'retweet_count', 'created_at','hashtags','quoted_hashtags','user_lat','user_lng'])\n",
    "\n",
    "    for tweet in tweepy.Cursor(api.search, q=data, count=100, lang='en').items():\n",
    "        hashtags=\"[]\"\n",
    "        quoted_hashtags=\"[]\"\n",
    "        #check if hashtags exist\n",
    "        if tweet.entities.get(\"hashtags\")!=[]:\n",
    "            hashtags = '['+','.join(pd.json_normalize(tweet.entities.get(\"hashtags\"))['text'].tolist())+']'\n",
    "        #check if it is quoted tweet and if hashtags exist in the quoted tweet\n",
    "        if hasattr(tweet, 'quoted_status') and tweet.quoted_status.entities.get(\"hashtags\")!=[]:\n",
    "            quoted_hashtags = '['+','.join(pd.json_normalize(tweet.quoted_status.entities.get(\"hashtags\"))['text'].tolist())+']'\n",
    "        #print(hashtags)\n",
    "        df.loc[filetweet_count, 'text'] = tweet.text\n",
    "        df.loc[filetweet_count, 'user.name'] = tweet.user.name\n",
    "        df.loc[filetweet_count, 'user.statuses_count'] = tweet.user.statuses_count\n",
    "        df.loc[filetweet_count, 'user.screen_name'] = tweet.user.screen_name\n",
    "        df.loc[filetweet_count, 'user.followers_count'] = tweet.user.followers_count\n",
    "        df.loc[filetweet_count, 'user.location'] = tweet.user.location\n",
    "        df.loc[filetweet_count, 'user.verified'] = tweet.user.verified\n",
    "        df.loc[filetweet_count, 'user.profile_image_url_https'] = tweet.user.profile_image_url_https\n",
    "        df.loc[filetweet_count, 'favorite_count'] = tweet.favorite_count\n",
    "        df.loc[filetweet_count, 'retweet_count'] = tweet.retweet_count\n",
    "        df.loc[filetweet_count, 'created_at'] = tweet.created_at\n",
    "        df.loc[filetweet_count, 'hashtags']=hashtags\n",
    "        df.loc[filetweet_count, 'quoted_hashtags']=quoted_hashtags\n",
    "        # process user location, check if user location exists first and then get lat/lng if it does, uncomment to get latitude longitude data from geolocator,\n",
    "        # please note this may take a long time depends on the number of tweet, see nominatim usage policy: https://operations.osmfoundation.org/policies/nominatim/, eg: No heavy uses (an absolute maximum of 1 request per second).\n",
    "        '''if tweet.user.location is not None:\n",
    "            coord = geolocator.geocode(tweet.user.location)\n",
    "            sleep(1)\n",
    "            if coord is not None:\n",
    "                df.loc[filetweet_count, 'User_lat'] = coord.latitude\n",
    "                df.loc[filetweet_count, 'User_lng'] = coord.longitude'''\n",
    "        # check number of tweets in the dataframe reaches tweetsperfile, save it to csv file if it has\n",
    "        if(filetweet_count>=tweetsperfile-1):\n",
    "            df.to_csv(file_name+\"_\"+str(fileno)+\".csv\",index = False)         \n",
    "            df = pd.DataFrame(columns = ['text', 'user.name', 'user.statuses_count','user.screen_name' \n",
    "                             'user.followers_count', 'user.location', 'user.verified','user.default_profile_image'\n",
    "                             'favorite_count', 'retweet_count', 'created_at','hashtags','quoted_hashtags','user_lat','user_lng'])\n",
    "\n",
    "            tweet_count+=1 # add 1 to total tweets count\n",
    "            fileno+=1 # add 1 to file number count\n",
    "            filetweet_count = 0 # reset tweets in file count\n",
    "        # check if total tweets retrieved reaches tweet_number, break search and save the remaining to csv file if it has\n",
    "        elif tweet_count >= tweet_number-1:\n",
    "            if(filetweet_count!=0):\n",
    "                df.to_csv(file_name+\"_\"+str(fileno)+\".csv\",index = False)\n",
    "            break         \n",
    "        else:\n",
    "            tweet_count+=1  # add 1 to total tweets count\n",
    "            filetweet_count+=1 # add 1 to tweets in file count\n",
    "            pass # keep searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set search terms for tweets and CSV file name to save the returned tweets. File number will be automatically added after file_name.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set twitter query for getting data, file_name to save data\n",
    "search_twitter_split(data = ['datascience','#datascience','bigdata','#bigdata','ai','#ai'], file_name = 'dba_tweets',tweet_number=tweet_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.2 Use Twitter Streaming API to get data\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tweepy and other required python libraries\n",
    "import tweepy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy import API\n",
    "import time\n",
    "import csv\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a function for customised Stream Listener class. Note: maxnotweets - maximum number of tweets to get, tweetsperfile - maximum number of tweets in each file; The function only get quoted tweets containing hashtags and quoted tweet hashtags, remove or change the condition if getting quoted tweets with hashtags and quoted tweet hashtags is not important.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customised Stream Listener class Note: The function below only get quoted tweets containing hashtags and quoted tweet hashtags, remove or change the condition if getting quoted tweets with hashtags and quoted tweet hashtags is not important\n",
    "class CustomListener(StreamListener):\n",
    "    \n",
    "    # init function for the listener api - not used in this use case, maxnotweets - maximum number of tweets to get, tweetsperfile - maximum number of tweets in each file\n",
    "    def __init__(self, api = None,maxnotweets=100,tweetsperfile=30):\n",
    "        self.api = api\n",
    "        self.counter = 0 # counter for no of tweet per file\n",
    "        self.maxcounter = 0 # counter for max no of tweets to get\n",
    "        self.max = maxnotweets # maximum number of total tweets to get\n",
    "        self.perfile = tweetsperfile # maximum number of tweets in each file\n",
    "        # create a file with 'tweets_' and the current time\n",
    "        self.filename = 'tweets'+'_'+time.strftime('%Y%m%d-%H%M%S')+'.csv' # first file name to save\n",
    "        # create a new file with the write handle and the file name  \n",
    "        csvFile = open(self.filename, 'w')\n",
    "        \n",
    "        # create a csv writer\n",
    "        csvWriter = csv.writer(csvFile)\n",
    "        \n",
    "        # use writeheader function to write the headers of the columns\n",
    "        self.writeHeader(csvWriter)\n",
    "        \n",
    "        # close the csv file\n",
    "        csvFile.close()\n",
    "    \n",
    "    #Define a function to write header\n",
    "    def writeHeader(self,csvWriter):\n",
    "        # Write a single row with the headers of the columns\n",
    "        csvWriter.writerow(['text',\n",
    "                            'created_at',\n",
    "                            'geo',\n",
    "                            'lang',\n",
    "                            'place',\n",
    "                            'hashtags',\n",
    "                            'coordinates',\n",
    "                            'user.description',\n",
    "                            'user.location',\n",
    "                            'user.id',\n",
    "                            'user.created_at',\n",
    "                            'user.url',\n",
    "                            'user.followers_count',\n",
    "                            'user.default_profile_image',\n",
    "                            'user.utc_offset',\n",
    "                            'user.name',\n",
    "                            'user.lang',\n",
    "                            'user.screen_name',\n",
    "                            'user.geo_enabled',\n",
    "                            'user.time_zone',\n",
    "                            'id',\n",
    "                            'favorite_count',\n",
    "                            'retweeted',\n",
    "                            'source',\n",
    "                            'favorited',\n",
    "                            'retweet_count',\n",
    "                            'quoted_status',\n",
    "                            'quoted_text',\n",
    "                            'quoted_hashtags'])\n",
    "    \n",
    "    \n",
    "    # Called when a new status/tweet arrives\n",
    "    def on_status(self, status):\n",
    "        # check if the counter of maximum total number of tweets is reached, return False to on_data function and break the streaming. \n",
    "        if self.maxcounter >= self.max:\n",
    "            return False\n",
    "        # open the current csv file \n",
    "        csvFile = open(self.filename, 'a')\n",
    "        \n",
    "        # check if the counter of maximum number of tweets per file is reached, output data to csv and create a new csv file\n",
    "        if self.counter >= self.perfile:\n",
    "            # close the csv file\n",
    "            csvFile.close()\n",
    "             # create a new file with 'tweet_' and the current time\n",
    "            self.filename = 'tweets'+'_'+time.strftime('%Y%m%d-%H%M%S')+'.csv'\n",
    "            # create a new file with write handle and the filename\n",
    "            csvFile = open(self.filename, 'w')\n",
    "            \n",
    "            # create a csv writer\n",
    "            csvWriter = csv.writer(csvFile)\n",
    "        \n",
    "             # use writeheader function to write the headers of the columns\n",
    "            self.writeHeader(csvWriter)\n",
    "            \n",
    "            # reset counter for maximum number of tweets per file\n",
    "            self.counter = 0\n",
    "            \n",
    "        \n",
    "        \n",
    "        # create a csv writer\n",
    "        csvWriter = csv.writer(csvFile)\n",
    "        try:\n",
    "            #check if it is quoted tweet and if hashtags exist in the quoted tweet and original tweet, remove or change this condition if quoted tweets with both hashtags and quoted tweet hashtags is not important\n",
    "            if hasattr(status, 'quoted_status') and status.entities.get(\"hashtags\")!=[] and status.quoted_status.entities.get(\"hashtags\")!=[]:\n",
    "                # transfer and normalize hashtags format \n",
    "                if status.entities.get(\"hashtags\")!=[]:\n",
    "                    hashtags = '['+','.join(pd.json_normalize(status.entities.get(\"hashtags\"))['text'].tolist())+']'\n",
    "                #check if it is quoted tweet and if hashtags exist in the quoted tweet\n",
    "                if hasattr(status, 'quoted_status') and status.quoted_status.entities.get(\"hashtags\")!=[]:\n",
    "                    quoted_hashtags = '['+','.join(pd.json_normalize(status.quoted_status.entities.get(\"hashtags\"))['text'].tolist())+']'\n",
    "                # write the tweet's information to the csv file\n",
    "                csvWriter.writerow([status.text,\n",
    "                                        status.created_at,\n",
    "                                        status.geo,\n",
    "                                        status.lang,\n",
    "                                        status.place,\n",
    "                                        hashtags,\n",
    "                                        status.coordinates,\n",
    "                                        status.user.description,\n",
    "                                        status.user.location,\n",
    "                                        status.user.id,\n",
    "                                        status.user.created_at,\n",
    "                                        status.user.url,\n",
    "                                        status.user.followers_count,\n",
    "                                        status.user.default_profile_image,\n",
    "                                        status.user.utc_offset,\n",
    "                                        status.user.name,\n",
    "                                        status.user.lang,\n",
    "                                        status.user.screen_name,\n",
    "                                        status.user.geo_enabled,\n",
    "                                        status.user.time_zone,\n",
    "                                        status.id,\n",
    "                                        status.favorite_count,\n",
    "                                        status.retweeted,\n",
    "                                        status.source,\n",
    "                                        status.favorited,\n",
    "                                        status.retweet_count,\n",
    "                                        status.quoted_status,\n",
    "                                        status.quoted_status.text,\n",
    "                                        quoted_hashtags\n",
    "                                   ])\n",
    "                # add 1 to counters\n",
    "                self.counter += 1\n",
    "                self.maxcounter += 1\n",
    "\n",
    "        # If error happens\n",
    "        except Exception as e:\n",
    "            # Print the error\n",
    "            print(e)\n",
    "            # and continue\n",
    "            pass    \n",
    "        # Close the csv file\n",
    "        csvFile.close()\n",
    "\n",
    "        return True\n",
    "\n",
    "    # called when a non-200/success status code is returned\n",
    "    def on_error(self, status_code):\n",
    "        # print the status code\n",
    "        print('Encountered error with status code:', status_code)\n",
    "        \n",
    "        # when status code is 401, which is the error for bad credentials\n",
    "        if status_code == 401:\n",
    "            # end the stream\n",
    "            return False\n",
    "\n",
    "    # called when a delete notice arrives for a status/tweet\n",
    "    def on_delete(self, status_id, user_id):\n",
    "        \n",
    "        # print delete message\n",
    "        print(\"Deleted notice\")\n",
    "        \n",
    "        return\n",
    "\n",
    "    # called when a limitation notice arrives/reach the rate limit\n",
    "    def on_limit(self, track):\n",
    "        \n",
    "        # print rate limiting message\n",
    "        print(\"Rate limited...\")\n",
    "        \n",
    "        # continue mining tweets\n",
    "        return True\n",
    "\n",
    "    # called when stream connection times out\n",
    "    def on_timeout(self):\n",
    "        \n",
    "        # print timeout message\n",
    "        print(sys.stderr, 'Timeout...')\n",
    "        \n",
    "        # wait 10 seconds\n",
    "        time.sleep(10)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    # called when twitter sends a disconnect notice\n",
    "    def on_disconnect(self):\n",
    "        \n",
    "        # print timeout message\n",
    "        print('Twitter disconnected...')\n",
    "       \n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a function for twitter streaming. Note: queries-defined search terms, maxnotweets - maximum number of tweets to get, tweetsperfile - maximum number of tweets in each file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for twitter streaming \n",
    "def stream_twitter(queries,maxnotweets=100,tweetsperfile=30):\n",
    "\n",
    "    \n",
    "    # set twitter API credentials information\n",
    "    consumer_key = '#######'\n",
    "    consumer_secret = '#######'\n",
    "    access_token = '#######'\n",
    "    access_token_secret = '#######'\n",
    "\n",
    "    # create the authorization handler object\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    \n",
    "    # set up the API with the authorization handler\n",
    "    api = API(auth)\n",
    "    # use the streaming listener\n",
    "    listener = CustomListener(api,maxnotweets,tweetsperfile)\n",
    "\n",
    "    \n",
    "    # create a streaming object with the custom listener and authorization\n",
    "    stream = Stream(auth, listener)\n",
    "\n",
    "    # run the streaming using the user defined search terms\n",
    "    stream.filter(track=queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start the twitter streaming for search terms. It may take a long period of time depends on the maxnotweets and how frequent the search terms are mentioned on twitter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the twitter streaming for search terms, eg 'digitalhumanities or #digitalhumanities', 'covid' or '#covid'\n",
    "stream_twitter(['covid', '#covid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "**How to get API Keys and Tokens for Twitter<br/>\n",
    "https://www.slickremix.com/docs/how-to-get-api-keys-and-tokens-for-twitter/<br/>\n",
    "Stream Tweets in Under 15 Lines of Code + Some Interactive Data Visualization<br/>\n",
    "https://dzone.com/articles/stream-tweets-the-easy-way-in-under-15-lines-of-co<br/>\n",
    "Twitter Firehose vs. Twitter API: What’s the difference and why should you care?<br/>\n",
    "https://brightplanet.com/2013/06/25/twitter-firehose-vs-twitter-api-whats-the-difference-and-why-should-you-care/ <br/>\n",
    "Twitter Data Visualisation<br/>\n",
    "https://nbviewer.jupyter.org/github/SantaDS/DataVisualisation/blob/master/TwitterDataAnalysis/twitter_data_analysis.ipynb<br/>\n",
    "Mine Twitter's Stream For Hashtags Or Words<br/>\n",
    "https://chrisalbon.com/python/other/mine_a_twitter_hashtags_and_words/<br/>\n",
    "Twitter Data Visualisation<br/>\n",
    "https://www.kaggle.com/tuncbileko/twitter-data-visualisation/<br/>\n",
    "Tweepy API Reference<br/>\n",
    "http://docs.tweepy.org/en/latest/api.html#API.search<br/>\n",
    "Tweepy Streaming<br/>\n",
    "https://github.com/tweepy/tweepy/blob/78d2883a922fa5232e8cdfab0c272c24b8ce37c4/tweepy/streaming.py<br/>\n",
    "Twitter API<br/>\n",
    "https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/user<br/>**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
